{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab10d6b1-09d4-4b35-b837-019fd9b87e4f",
   "metadata": {},
   "source": [
    "# ğŸ”¥ğŸ PySpark in Python: Step 1ï¸âƒ£ - Installation and Setup in Jupyter Notebook ğŸš€  \n",
    "\n",
    "## â“ Want to learn PySpark but donâ€™t want to pay for expensive tools?  \n",
    "ğŸ™ƒ Good news! You can install and configure it in Jupyter Notebook for free.  \n",
    "\n",
    "## ğŸ‘‰ Solution  \n",
    "Weâ€™ll use `findspark` to make PySpark work locally without complex configurations.  \n",
    "\n",
    "## ğŸ”§ How does it work?  \n",
    "`findspark` allows us to import PySpark into a notebook without manually setting environment variables.  \n",
    "\n",
    "## ğŸ” Why does it matter?  \n",
    "âœ”ï¸ **Experiment with Big Data** on your PC at no cost.  \n",
    "âœ”ï¸ Learn PySpark before moving to **AWS EMR** or **Databricks** clusters.  \n",
    "âœ”ï¸ Perfect for quick testing and **agile development**.  \n",
    "\n",
    "## âš ï¸ Limitations of running PySpark locally  \n",
    "ğŸ”¹ Local PySpark doesnâ€™t leverage distributed computing like a real cluster.  \n",
    "ğŸ”¹ Performance is limited by your PCâ€™s resources (CPU, RAM).  \n",
    "ğŸ”¹ No horizontal scaling, which can cause bottlenecks with large datasets.  \n",
    "ğŸ”¹ Some Spark optimizations, like advanced parallelization, donâ€™t work the same way as in a distributed environment.  \n",
    "\n",
    "## âœ¨ Real-world example  \n",
    "Imagine analyzing massive **social media data** in Spark without relying on expensive infrastructure. With this setup, you can process tweets on your laptop before scaling.  \n",
    "\n",
    "## âš™ï¸ Business impact  \n",
    "ğŸ“Š **Companies can test Machine Learning models in PySpark locally before investing in high-performance servers.**  \n",
    "\n",
    "## ğŸ“Š Code summary  \n",
    "1ï¸âƒ£ Install `PySpark` and `findspark`.  \n",
    "2ï¸âƒ£ Configure `findspark` to locate Spark.  \n",
    "3ï¸âƒ£ Create a **Spark session** in **Jupyter Notebook**.  \n",
    "4ï¸âƒ£ Test the setup by loading a **sample DataFrame**.  \n",
    "\n",
    "ğŸ”— **GitHub repository:** [PySparkForAll - Step 1ï¸âƒ£](https://github.com/jcombari/PySparkForAll/blob/main/20250320_Step%201%EF%B8%8F%E2%83%A3%20-%20Installation%20and%20Setup%20in%20Jupyter%20Notebook%20%F0%9F%9A%80.ipynb)  \n",
    "\n",
    "ğŸ’­ **Have you used PySpark before? Share your experience or ask any questions.**  \n",
    "\n",
    "ğŸ”‘ #PySpark #BigData #MachineLearning #Python #JupyterNotebook #DataScience  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¥ğŸ PySpark en Python: Paso 1ï¸âƒ£ - InstalaciÃ³n y ConfiguraciÃ³n en Jupyter Notebook ğŸš€  \n",
    "\n",
    "## â“ Â¿Quieres aprender PySpark pero no quieres pagar herramientas costosas?  \n",
    "ğŸ™ƒ Â¡Buenas noticias! Puedes instalarlo y configurarlo en Jupyter Notebook sin gastar nada.  \n",
    "\n",
    "## ğŸ‘‰ SoluciÃ³n  \n",
    "Usaremos `findspark` para que PySpark funcione en local y sin configuraciones complicadas.  \n",
    "\n",
    "## ğŸ”§ Â¿CÃ³mo funciona?  \n",
    "`findspark` nos permite importar PySpark en un notebook sin necesidad de configurar variables de entorno manualmente.  \n",
    "\n",
    "## ğŸ” Â¿Por quÃ© importa?  \n",
    "âœ”ï¸ Permite experimentar con **Big Data** en tu PC sin costos adicionales.  \n",
    "âœ”ï¸ Facilita el aprendizaje antes de pasar a clusters como **AWS EMR** o **Databricks**.  \n",
    "âœ”ï¸ Ideal para pruebas rÃ¡pidas y desarrollo Ã¡gil.  \n",
    "\n",
    "## âš ï¸ Limitaciones del uso en modo local  \n",
    "ğŸ”¹ PySpark en local no aprovecha la computaciÃ³n distribuida como en un clÃºster real.  \n",
    "ğŸ”¹ El rendimiento estÃ¡ limitado por los recursos de tu PC (CPU, RAM).  \n",
    "ğŸ”¹ No permite el escalado horizontal, lo que puede generar cuellos de botella en grandes volÃºmenes de datos.  \n",
    "ğŸ”¹ Algunas optimizaciones de Spark, como la paralelizaciÃ³n avanzada, no funcionan de la misma manera que en un entorno distribuido.  \n",
    "\n",
    "## âœ¨ Ejemplo real  \n",
    "Imagina que quieres analizar grandes volÃºmenes de datos de redes sociales en Spark sin depender de infraestructuras pagas. Con esta configuraciÃ³n, puedes procesar tweets en tu laptop antes de escalar.  \n",
    "\n",
    "## âš™ï¸ Impacto en el negocio  \n",
    "ğŸ“Š **Las empresas pueden probar modelos de Machine Learning en PySpark localmente antes de invertir en servidores de alto rendimiento.**  \n",
    "\n",
    "## ğŸ“Š Resumen del cÃ³digo  \n",
    "1ï¸âƒ£ Instalamos `PySpark` y `findspark`.  \n",
    "2ï¸âƒ£ Configuramos `findspark` para localizar Spark.  \n",
    "3ï¸âƒ£ Creamos una sesiÃ³n de Spark en **Jupyter Notebook**.  \n",
    "4ï¸âƒ£ Probamos que funciona cargando un **DataFrame de ejemplo**.  \n",
    "\n",
    "ğŸ”— **Repositorio en GitHub:** [PySparkForAll - Paso 1ï¸âƒ£](https://github.com/jcombari/PySparkForAll/blob/main/20250320_Step%201%EF%B8%8F%E2%83%A3%20-%20Installation%20and%20Setup%20in%20Jupyter%20Notebook%20%F0%9F%9A%80.ipynb)  \n",
    "\n",
    "ğŸ’­ **Â¿Has usado PySpark antes? Comenta tu experiencia o pregunta lo que necesites.**  \n",
    "\n",
    "ğŸ”‘ #PySpark #BigData #MachineLearning #Python #JupyterNotebook #DataScience  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
